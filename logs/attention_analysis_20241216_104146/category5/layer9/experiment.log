WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/main.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/main.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2024-12-16 11:06:51.529215: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2024-12-16 11:06:51.551500: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2024-12-16 11:06:51.552017: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3cf9090 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-12-16 11:06:51.552027: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-12-16 11:06:51.554681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-12-16 11:06:51.683376: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3cdfc00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-12-16 11:06:51.683402: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-12-16 11:06:51.683871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:2f:00.0
2024-12-16 11:06:51.685001: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.686341: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.687347: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.688295: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.689238: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.690168: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.691095: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
2024-12-16 11:06:51.691105: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-12-16 11:06:51.691124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-16 11:06:51.691129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-12-16 11:06:51.691132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/main.py:69: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/vgg_16.py:61: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/vgg_16.py:87: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/vgg_16.py:15: sigmoid_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.sigmoid_cross_entropy instead. Note that the order of the predictions and labels arguments has been changed.
WARNING:tensorflow:From /ext3/envs/vgg16_env/lib/python2.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ext3/envs/vgg16_env/lib/python2.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:322: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.compute_weighted_loss instead.
WARNING:tensorflow:From /ext3/envs/vgg16_env/lib/python2.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /ext3/envs/vgg16_env/lib/python2.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.add_loss instead.
WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/vgg_16.py:17: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/vgg_16.py:25: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /scratch/hy2611/CNN_attention/main/new/main.py:82: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.


Verifying paths:
tc_path: /scratch/hy2611/CNN_attention/Data/VGG16/object_GradsTCs exists
weight_path: /scratch/hy2611/CNN_attention/Data/VGG16 exists
image_path: /scratch/hy2611/CNN_attention/Data/VGG16/images exists
save_path: /scratch/hy2611/CNN_attention/Data/VGG16 exists

Initializing model...
('c11', [1, 224, 224, 64])
('c12', [1, 224, 224, 64])
('c21', [1, 112, 112, 128])
('c22', [1, 112, 112, 128])
('c31', [1, 56, 56, 256])
('c32', [1, 56, 56, 256])
('c33', [1, 56, 56, 256])
('c41', [1, 28, 28, 512])
('c42', [1, 28, 28, 512])
('c43', [1, 28, 28, 512])
('c51', [1, 14, 14, 512])
('c52', [1, 14, 14, 512])
('c53', [1, 14, 14, 512])
(0, 'conv1_1_W', (3, 3, 3, 64))
(1, 'conv1_1_b', (64,))
(2, 'conv1_2_W', (3, 3, 64, 64))
(3, 'conv1_2_b', (64,))
(4, 'conv2_1_W', (3, 3, 64, 128))
(5, 'conv2_1_b', (128,))
(6, 'conv2_2_W', (3, 3, 128, 128))
(7, 'conv2_2_b', (128,))
(8, 'conv3_1_W', (3, 3, 128, 256))
(9, 'conv3_1_b', (256,))
(10, 'conv3_2_W', (3, 3, 256, 256))
(11, 'conv3_2_b', (256,))
(12, 'conv3_3_W', (3, 3, 256, 256))
(13, 'conv3_3_b', (256,))
(14, 'conv4_1_W', (3, 3, 256, 512))
(15, 'conv4_1_b', (512,))
(16, 'conv4_2_W', (3, 3, 512, 512))
(17, 'conv4_2_b', (512,))
(18, 'conv4_3_W', (3, 3, 512, 512))
(19, 'conv4_3_b', (512,))
(20, 'conv5_1_W', (3, 3, 512, 512))
(21, 'conv5_1_b', (512,))
(22, 'conv5_2_W', (3, 3, 512, 512))
(23, 'conv5_2_b', (512,))
(24, 'conv5_3_W', (3, 3, 512, 512))
(25, 'conv5_3_b', (512,))
(26, 'fc6_W', (25088, 4096))
(27, 'fc6_b', (4096,))
(28, 'fc7_W', (4096, 4096))
(29, 'fc7_b', (4096,))
Checking checkpoint files:
Data file: /scratch/hy2611/CNN_attention/Data/VGG16/catbins/catbin_5.ckpt.data-00000-of-00001 - Found
Index file: /scratch/hy2611/CNN_attention/Data/VGG16/catbins/catbin_5.ckpt.index - Found
Loading checkpoint from: /scratch/hy2611/CNN_attention/Data/VGG16/catbins/catbin_5.ckpt
Checkpoint loaded successfully

Initializing components...
Found tuning curves file: /scratch/hy2611/CNN_attention/Data/VGG16/object_GradsTCs/featvecs20_train35_c.txt

Loading category 5 data...
Original positive images shape: (2, 224, 224, 3)

Processing data...

Processing attention strength 0.0
Attention vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

Processing batch 1/2
Attention strength: 0.0
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.40286, std: 0.12046

Metrics for layer 0:
  pearson_correlation: -0.0027
  kl_divergence: -4383.4224
  ssim: 0.0522
  iou: 0.1398
Layer 0 metrics:
  pearson_correlation: -0.0027
  kl_divergence: -4383.4224
  ssim: 0.0522
  iou: 0.1398

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.46678, std: 0.11471

Metrics for layer 1:
  pearson_correlation: 0.0041
  kl_divergence: -4892.0093
  ssim: 0.0483
  iou: 0.1460
Layer 1 metrics:
  pearson_correlation: 0.0041
  kl_divergence: -4892.0093
  ssim: 0.0483
  iou: 0.1460

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.49420, std: 0.13354

Metrics for layer 2:
  pearson_correlation: 0.0077
  kl_divergence: -1475.6335
  ssim: 0.0498
  iou: 0.1443
Layer 2 metrics:
  pearson_correlation: 0.0077
  kl_divergence: -1475.6335
  ssim: 0.0498
  iou: 0.1443

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.47496, std: 0.13873

Metrics for layer 3:
  pearson_correlation: 0.0030
  kl_divergence: -1427.9312
  ssim: 0.0493
  iou: 0.1422
Layer 3 metrics:
  pearson_correlation: 0.0030
  kl_divergence: -1427.9312
  ssim: 0.0493
  iou: 0.1422

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.35615, std: 0.12679

Metrics for layer 4:
  pearson_correlation: 0.0217
  kl_divergence: -258.9319
  ssim: 0.0989
  iou: 0.1420
Layer 4 metrics:
  pearson_correlation: 0.0217
  kl_divergence: -258.9319
  ssim: 0.0989
  iou: 0.1420

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.49967, std: 0.14725

Metrics for layer 5:
  pearson_correlation: -0.0387
  kl_divergence: -386.0103
  ssim: 0.0428
  iou: 0.1305
Layer 5 metrics:
  pearson_correlation: -0.0387
  kl_divergence: -386.0103
  ssim: 0.0428
  iou: 0.1305

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.46884, std: 0.14095

Metrics for layer 6:
  pearson_correlation: 0.0102
  kl_divergence: -369.4884
  ssim: 0.0593
  iou: 0.1470
Layer 6 metrics:
  pearson_correlation: 0.0102
  kl_divergence: -369.4884
  ssim: 0.0593
  iou: 0.1470

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.45305, std: 0.14404

Metrics for layer 7:
  pearson_correlation: 0.0538
  kl_divergence: -85.3420
  ssim: 0.0911
  iou: 0.1395
Layer 7 metrics:
  pearson_correlation: 0.0538
  kl_divergence: -85.3420
  ssim: 0.0911
  iou: 0.1395

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.47342, std: 0.15645

Metrics for layer 8:
  pearson_correlation: 0.0134
  kl_divergence: -88.3023
  ssim: 0.0678
  iou: 0.1667
Layer 8 metrics:
  pearson_correlation: 0.0134
  kl_divergence: -88.3023
  ssim: 0.0678
  iou: 0.1667

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.49701, std: 0.16740

Metrics for layer 9:
  pearson_correlation: -0.0389
  kl_divergence: -89.5686
  ssim: 0.0183
  iou: 0.1737
Layer 9 metrics:
  pearson_correlation: -0.0389
  kl_divergence: -89.5686
  ssim: 0.0183
  iou: 0.1737

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.39263, std: 0.18016

Metrics for layer 10:
  pearson_correlation: 0.0279
  kl_divergence: -14.4285
  ssim: 0.0086
  iou: 0.1264
Layer 10 metrics:
  pearson_correlation: 0.0279
  kl_divergence: -14.4285
  ssim: 0.0086
  iou: 0.1264

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.49125, std: 0.19985

Metrics for layer 11:
  pearson_correlation: 0.0648
  kl_divergence: -20.8443
  ssim: 0.0073
  iou: 0.1807
Layer 11 metrics:
  pearson_correlation: 0.0648
  kl_divergence: -20.8443
  ssim: 0.0073
  iou: 0.1807

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.39890, std: 0.18666

Metrics for layer 12:
  pearson_correlation: 0.0141
  kl_divergence: -7.7011
  ssim: 0.0473
  iou: 0.1529
Layer 12 metrics:
  pearson_correlation: 0.0141
  kl_divergence: -7.7011
  ssim: 0.0473
  iou: 0.1529
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_0_strength_0.0_results.npy

Processing batch 2/2
Attention strength: 0.0
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.42611, std: 0.12730

Metrics for layer 0:
  pearson_correlation: -0.0010
  kl_divergence: -3763.3362
  ssim: 0.0314
  iou: 0.1445
Layer 0 metrics:
  pearson_correlation: -0.0010
  kl_divergence: -3763.3362
  ssim: 0.0314
  iou: 0.1445

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.42735, std: 0.12081

Metrics for layer 1:
  pearson_correlation: -0.0059
  kl_divergence: -3777.7229
  ssim: 0.0331
  iou: 0.1370
Layer 1 metrics:
  pearson_correlation: -0.0059
  kl_divergence: -3777.7229
  ssim: 0.0331
  iou: 0.1370

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.44472, std: 0.13186

Metrics for layer 2:
  pearson_correlation: 0.0100
  kl_divergence: -1337.6969
  ssim: 0.0507
  iou: 0.1510
Layer 2 metrics:
  pearson_correlation: 0.0100
  kl_divergence: -1337.6969
  ssim: 0.0507
  iou: 0.1510

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.44813, std: 0.13317

Metrics for layer 3:
  pearson_correlation: -0.0035
  kl_divergence: -1342.8351
  ssim: 0.0484
  iou: 0.1422
Layer 3 metrics:
  pearson_correlation: -0.0035
  kl_divergence: -1342.8351
  ssim: 0.0484
  iou: 0.1422

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.51748, std: 0.15908

Metrics for layer 4:
  pearson_correlation: 0.0227
  kl_divergence: -398.9630
  ssim: 0.0456
  iou: 0.1445
Layer 4 metrics:
  pearson_correlation: 0.0227
  kl_divergence: -398.9630
  ssim: 0.0456
  iou: 0.1445

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.46289, std: 0.14715

Metrics for layer 5:
  pearson_correlation: -0.0102
  kl_divergence: -367.5609
  ssim: 0.0491
  iou: 0.1445
Layer 5 metrics:
  pearson_correlation: -0.0102
  kl_divergence: -367.5609
  ssim: 0.0491
  iou: 0.1445

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.49883, std: 0.15082

Metrics for layer 6:
  pearson_correlation: -0.0215
  kl_divergence: -398.7167
  ssim: 0.0429
  iou: 0.1462
Layer 6 metrics:
  pearson_correlation: -0.0215
  kl_divergence: -398.7167
  ssim: 0.0429
  iou: 0.1462

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.48732, std: 0.15027

Metrics for layer 7:
  pearson_correlation: 0.0059
  kl_divergence: -88.0546
  ssim: 0.0293
  iou: 0.1529
Layer 7 metrics:
  pearson_correlation: 0.0059
  kl_divergence: -88.0546
  ssim: 0.0293
  iou: 0.1529

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.54574, std: 0.15111

Metrics for layer 8:
  pearson_correlation: 0.0263
  kl_divergence: -95.1174
  ssim: 0.0467
  iou: 0.1232
Layer 8 metrics:
  pearson_correlation: 0.0263
  kl_divergence: -95.1174
  ssim: 0.0467
  iou: 0.1232

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.50018, std: 0.15994

Metrics for layer 9:
  pearson_correlation: -0.0308
  kl_divergence: -89.2089
  ssim: 0.0338
  iou: 0.1598
Layer 9 metrics:
  pearson_correlation: -0.0308
  kl_divergence: -89.2089
  ssim: 0.0338
  iou: 0.1598

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.50459, std: 0.17585

Metrics for layer 10:
  pearson_correlation: -0.1230
  kl_divergence: -19.6301
  ssim: -0.0734
  iou: 0.0889
Layer 10 metrics:
  pearson_correlation: -0.1230
  kl_divergence: -19.6301
  ssim: -0.0734
  iou: 0.0889

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.51337, std: 0.18047

Metrics for layer 11:
  pearson_correlation: 0.0268
  kl_divergence: -19.6920
  ssim: 0.1285
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: 0.0268
  kl_divergence: -19.6920
  ssim: 0.1285
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.48844, std: 0.19435

Metrics for layer 12:
  pearson_correlation: -0.0870
  kl_divergence: -13.3770
  ssim: -0.0191
  iou: 0.1395
Layer 12 metrics:
  pearson_correlation: -0.0870
  kl_divergence: -13.3770
  ssim: -0.0191
  iou: 0.1395
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_1_strength_0.0_results.npy

Processing attention strength 0.4
Attention vector: [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0. ]

Processing batch 1/2
Attention strength: 0.4
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.43795, std: 0.12608

Metrics for layer 0:
  pearson_correlation: 0.0005
  kl_divergence: -4650.8706
  ssim: 0.0448
  iou: 0.1412
Layer 0 metrics:
  pearson_correlation: 0.0005
  kl_divergence: -4650.8706
  ssim: 0.0448
  iou: 0.1412

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.44182, std: 0.12250

Metrics for layer 1:
  pearson_correlation: -0.0001
  kl_divergence: -4690.4233
  ssim: 0.0470
  iou: 0.1440
Layer 1 metrics:
  pearson_correlation: -0.0001
  kl_divergence: -4690.4233
  ssim: 0.0470
  iou: 0.1440

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.43659, std: 0.12536

Metrics for layer 2:
  pearson_correlation: 0.0025
  kl_divergence: -1342.8820
  ssim: 0.0600
  iou: 0.1420
Layer 2 metrics:
  pearson_correlation: 0.0025
  kl_divergence: -1342.8820
  ssim: 0.0600
  iou: 0.1420

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.45657, std: 0.12827

Metrics for layer 3:
  pearson_correlation: -0.0035
  kl_divergence: -1389.2878
  ssim: 0.0543
  iou: 0.1487
Layer 3 metrics:
  pearson_correlation: -0.0035
  kl_divergence: -1389.2878
  ssim: 0.0543
  iou: 0.1487

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.42359, std: 0.14151

Metrics for layer 4:
  pearson_correlation: -0.0046
  kl_divergence: -325.0127
  ssim: 0.0581
  iou: 0.1321
Layer 4 metrics:
  pearson_correlation: -0.0046
  kl_divergence: -325.0127
  ssim: 0.0581
  iou: 0.1321

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.50492, std: 0.16296

Metrics for layer 5:
  pearson_correlation: 0.0166
  kl_divergence: -389.4326
  ssim: 0.0548
  iou: 0.1379
Layer 5 metrics:
  pearson_correlation: 0.0166
  kl_divergence: -389.4326
  ssim: 0.0548
  iou: 0.1379

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.49541, std: 0.14110

Metrics for layer 6:
  pearson_correlation: 0.0049
  kl_divergence: -386.0701
  ssim: 0.0593
  iou: 0.1354
Layer 6 metrics:
  pearson_correlation: 0.0049
  kl_divergence: -386.0701
  ssim: 0.0593
  iou: 0.1354

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.55919, std: 0.15965

Metrics for layer 7:
  pearson_correlation: 0.0588
  kl_divergence: -107.1927
  ssim: 0.0602
  iou: 0.1843
Layer 7 metrics:
  pearson_correlation: 0.0588
  kl_divergence: -107.1927
  ssim: 0.0602
  iou: 0.1843

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.53467, std: 0.14349

Metrics for layer 8:
  pearson_correlation: 0.0013
  kl_divergence: -103.5453
  ssim: 0.0703
  iou: 0.1529
Layer 8 metrics:
  pearson_correlation: 0.0013
  kl_divergence: -103.5453
  ssim: 0.0703
  iou: 0.1529

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.48747, std: 0.13893

Metrics for layer 9:
  pearson_correlation: -0.0057
  kl_divergence: -91.5723
  ssim: 0.0693
  iou: 0.1529
Layer 9 metrics:
  pearson_correlation: -0.0057
  kl_divergence: -91.5723
  ssim: 0.0693
  iou: 0.1529

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.44224, std: 0.17799

Metrics for layer 10:
  pearson_correlation: 0.0883
  kl_divergence: -15.3312
  ssim: 0.0574
  iou: 0.1807
Layer 10 metrics:
  pearson_correlation: 0.0883
  kl_divergence: -15.3312
  ssim: 0.0574
  iou: 0.1807

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.54386, std: 0.18709

Metrics for layer 11:
  pearson_correlation: 0.0434
  kl_divergence: -24.7532
  ssim: 0.0234
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: 0.0434
  kl_divergence: -24.7532
  ssim: 0.0234
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.46522, std: 0.17636

Metrics for layer 12:
  pearson_correlation: -0.0276
  kl_divergence: -18.8316
  ssim: 0.0593
  iou: 0.1395
Layer 12 metrics:
  pearson_correlation: -0.0276
  kl_divergence: -18.8316
  ssim: 0.0593
  iou: 0.1395
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_0_strength_0.4_results.npy

Processing batch 2/2
Attention strength: 0.4
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.43533, std: 0.12038

Metrics for layer 0:
  pearson_correlation: 0.0026
  kl_divergence: -3821.0679
  ssim: 0.0334
  iou: 0.1455
Layer 0 metrics:
  pearson_correlation: 0.0026
  kl_divergence: -3821.0679
  ssim: 0.0334
  iou: 0.1455

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.44034, std: 0.11724

Metrics for layer 1:
  pearson_correlation: 0.0016
  kl_divergence: -3848.8057
  ssim: 0.0340
  iou: 0.1462
Layer 1 metrics:
  pearson_correlation: 0.0016
  kl_divergence: -3848.8057
  ssim: 0.0340
  iou: 0.1462

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.44307, std: 0.14225

Metrics for layer 2:
  pearson_correlation: 0.0030
  kl_divergence: -1324.9209
  ssim: 0.0440
  iou: 0.1422
Layer 2 metrics:
  pearson_correlation: 0.0030
  kl_divergence: -1324.9209
  ssim: 0.0440
  iou: 0.1422

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.49006, std: 0.14540

Metrics for layer 3:
  pearson_correlation: 0.0026
  kl_divergence: -1422.7703
  ssim: 0.0392
  iou: 0.1420
Layer 3 metrics:
  pearson_correlation: 0.0026
  kl_divergence: -1422.7703
  ssim: 0.0392
  iou: 0.1420

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.46622, std: 0.13213

Metrics for layer 4:
  pearson_correlation: 0.0025
  kl_divergence: -380.3528
  ssim: 0.0621
  iou: 0.1272
Layer 4 metrics:
  pearson_correlation: 0.0025
  kl_divergence: -380.3528
  ssim: 0.0621
  iou: 0.1272

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.48809, std: 0.13816

Metrics for layer 5:
  pearson_correlation: 0.0359
  kl_divergence: -397.5771
  ssim: 0.0590
  iou: 0.1329
Layer 5 metrics:
  pearson_correlation: 0.0359
  kl_divergence: -397.5771
  ssim: 0.0590
  iou: 0.1329

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.41728, std: 0.13135

Metrics for layer 6:
  pearson_correlation: 0.0022
  kl_divergence: -335.9899
  ssim: 0.0551
  iou: 0.1429
Layer 6 metrics:
  pearson_correlation: 0.0022
  kl_divergence: -335.9899
  ssim: 0.0551
  iou: 0.1429

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.45690, std: 0.16813

Metrics for layer 7:
  pearson_correlation: -0.0068
  kl_divergence: -81.2287
  ssim: 0.0296
  iou: 0.1462
Layer 7 metrics:
  pearson_correlation: -0.0068
  kl_divergence: -81.2287
  ssim: 0.0296
  iou: 0.1462

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.50153, std: 0.16107

Metrics for layer 8:
  pearson_correlation: -0.0390
  kl_divergence: -88.4880
  ssim: 0.0219
  iou: 0.1462
Layer 8 metrics:
  pearson_correlation: -0.0390
  kl_divergence: -88.4880
  ssim: 0.0219
  iou: 0.1462

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.52271, std: 0.14755

Metrics for layer 9:
  pearson_correlation: 0.0113
  kl_divergence: -91.6825
  ssim: 0.0322
  iou: 0.1429
Layer 9 metrics:
  pearson_correlation: 0.0113
  kl_divergence: -91.6825
  ssim: 0.0322
  iou: 0.1429

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.43772, std: 0.17943

Metrics for layer 10:
  pearson_correlation: 0.1407
  kl_divergence: -15.7853
  ssim: 0.1418
  iou: 0.1951
Layer 10 metrics:
  pearson_correlation: 0.1407
  kl_divergence: -15.7853
  ssim: 0.1418
  iou: 0.1951

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.56769, std: 0.18808

Metrics for layer 11:
  pearson_correlation: -0.0166
  kl_divergence: -24.6752
  ssim: -0.0441
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: -0.0166
  kl_divergence: -24.6752
  ssim: -0.0441
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.51408, std: 0.19401

Metrics for layer 12:
  pearson_correlation: 0.1197
  kl_divergence: -20.7701
  ssim: 0.1289
  iou: 0.1529
Layer 12 metrics:
  pearson_correlation: 0.1197
  kl_divergence: -20.7701
  ssim: 0.1289
  iou: 0.1529
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_1_strength_0.4_results.npy

Processing attention strength 0.8
Attention vector: [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 0.  0.  0. ]

Processing batch 1/2
Attention strength: 0.8
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.40672, std: 0.11824

Metrics for layer 0:
  pearson_correlation: 0.0007
  kl_divergence: -4421.4219
  ssim: 0.0531
  iou: 0.1423
Layer 0 metrics:
  pearson_correlation: 0.0007
  kl_divergence: -4421.4219
  ssim: 0.0531
  iou: 0.1423

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.45917, std: 0.12120

Metrics for layer 1:
  pearson_correlation: 0.0034
  kl_divergence: -4823.5796
  ssim: 0.0465
  iou: 0.1407
Layer 1 metrics:
  pearson_correlation: 0.0034
  kl_divergence: -4823.5796
  ssim: 0.0465
  iou: 0.1407

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.43420, std: 0.13668

Metrics for layer 2:
  pearson_correlation: 0.0097
  kl_divergence: -1325.5842
  ssim: 0.0567
  iou: 0.1389
Layer 2 metrics:
  pearson_correlation: 0.0097
  kl_divergence: -1325.5842
  ssim: 0.0567
  iou: 0.1389

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.48515, std: 0.12587

Metrics for layer 3:
  pearson_correlation: 0.0179
  kl_divergence: -1462.7189
  ssim: 0.0549
  iou: 0.1443
Layer 3 metrics:
  pearson_correlation: 0.0179
  kl_divergence: -1462.7189
  ssim: 0.0549
  iou: 0.1443

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.45981, std: 0.12561

Metrics for layer 4:
  pearson_correlation: 0.0043
  kl_divergence: -364.2590
  ssim: 0.0672
  iou: 0.1462
Layer 4 metrics:
  pearson_correlation: 0.0043
  kl_divergence: -364.2590
  ssim: 0.0672
  iou: 0.1462

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.48971, std: 0.14611

Metrics for layer 5:
  pearson_correlation: 0.0294
  kl_divergence: -382.0869
  ssim: 0.0755
  iou: 0.1563
Layer 5 metrics:
  pearson_correlation: 0.0294
  kl_divergence: -382.0869
  ssim: 0.0755
  iou: 0.1563

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.45612, std: 0.14278

Metrics for layer 6:
  pearson_correlation: 0.0129
  kl_divergence: -349.4545
  ssim: 0.0491
  iou: 0.1589
Layer 6 metrics:
  pearson_correlation: 0.0129
  kl_divergence: -349.4545
  ssim: 0.0491
  iou: 0.1589

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.44172, std: 0.16216

Metrics for layer 7:
  pearson_correlation: -0.0241
  kl_divergence: -80.3054
  ssim: 0.0372
  iou: 0.1105
Layer 7 metrics:
  pearson_correlation: -0.0241
  kl_divergence: -80.3054
  ssim: 0.0372
  iou: 0.1105

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.47436, std: 0.16335

Metrics for layer 8:
  pearson_correlation: -0.0190
  kl_divergence: -87.4456
  ssim: 0.0577
  iou: 0.1429
Layer 8 metrics:
  pearson_correlation: -0.0190
  kl_divergence: -87.4456
  ssim: 0.0577
  iou: 0.1429

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.52142, std: 0.14507

Metrics for layer 9:
  pearson_correlation: 0.0118
  kl_divergence: -99.4358
  ssim: 0.0276
  iou: 0.1807
Layer 9 metrics:
  pearson_correlation: 0.0118
  kl_divergence: -99.4358
  ssim: 0.0276
  iou: 0.1807

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.43528, std: 0.21565

Metrics for layer 10:
  pearson_correlation: -0.0181
  kl_divergence: -14.8618
  ssim: -0.0168
  iou: 0.1011
Layer 10 metrics:
  pearson_correlation: -0.0181
  kl_divergence: -14.8618
  ssim: -0.0168
  iou: 0.1011

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.42228, std: 0.17430

Metrics for layer 11:
  pearson_correlation: 0.0130
  kl_divergence: -10.4718
  ssim: 0.1073
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: 0.0130
  kl_divergence: -10.4718
  ssim: 0.1073
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.45824, std: 0.18530

Metrics for layer 12:
  pearson_correlation: -0.0385
  kl_divergence: -16.5397
  ssim: -0.0409
  iou: 0.2099
Layer 12 metrics:
  pearson_correlation: -0.0385
  kl_divergence: -16.5397
  ssim: -0.0409
  iou: 0.2099
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_0_strength_0.8_results.npy

Processing batch 2/2
Attention strength: 0.8
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.43439, std: 0.12185

Metrics for layer 0:
  pearson_correlation: 0.0028
  kl_divergence: -3813.8345
  ssim: 0.0326
  iou: 0.1443
Layer 0 metrics:
  pearson_correlation: 0.0028
  kl_divergence: -3813.8345
  ssim: 0.0326
  iou: 0.1443

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.44296, std: 0.12743

Metrics for layer 1:
  pearson_correlation: 0.0053
  kl_divergence: -3846.6772
  ssim: 0.0304
  iou: 0.1430
Layer 1 metrics:
  pearson_correlation: 0.0053
  kl_divergence: -3846.6772
  ssim: 0.0304
  iou: 0.1430

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.43214, std: 0.12537

Metrics for layer 2:
  pearson_correlation: 0.0213
  kl_divergence: -1318.1863
  ssim: 0.0603
  iou: 0.1510
Layer 2 metrics:
  pearson_correlation: 0.0213
  kl_divergence: -1318.1863
  ssim: 0.0603
  iou: 0.1510

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.47193, std: 0.13070

Metrics for layer 3:
  pearson_correlation: -0.0081
  kl_divergence: -1391.3364
  ssim: 0.0452
  iou: 0.1437
Layer 3 metrics:
  pearson_correlation: -0.0081
  kl_divergence: -1391.3364
  ssim: 0.0452
  iou: 0.1437

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.43871, std: 0.13856

Metrics for layer 4:
  pearson_correlation: 0.0166
  kl_divergence: -352.5057
  ssim: 0.0562
  iou: 0.1512
Layer 4 metrics:
  pearson_correlation: 0.0166
  kl_divergence: -352.5057
  ssim: 0.0562
  iou: 0.1512

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.48476, std: 0.15002

Metrics for layer 5:
  pearson_correlation: 0.0089
  kl_divergence: -391.6406
  ssim: 0.0538
  iou: 0.1387
Layer 5 metrics:
  pearson_correlation: 0.0089
  kl_divergence: -391.6406
  ssim: 0.0538
  iou: 0.1387

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.47483, std: 0.14417

Metrics for layer 6:
  pearson_correlation: -0.0202
  kl_divergence: -382.0146
  ssim: 0.0464
  iou: 0.1445
Layer 6 metrics:
  pearson_correlation: -0.0202
  kl_divergence: -382.0146
  ssim: 0.0464
  iou: 0.1445

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.56068, std: 0.14564

Metrics for layer 7:
  pearson_correlation: -0.0224
  kl_divergence: -97.1962
  ssim: 0.0306
  iou: 0.1496
Layer 7 metrics:
  pearson_correlation: -0.0224
  kl_divergence: -97.1962
  ssim: 0.0306
  iou: 0.1496

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.46241, std: 0.17899

Metrics for layer 8:
  pearson_correlation: 0.0389
  kl_divergence: -83.5309
  ssim: 0.0531
  iou: 0.1200
Layer 8 metrics:
  pearson_correlation: 0.0389
  kl_divergence: -83.5309
  ssim: 0.0531
  iou: 0.1200

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.50559, std: 0.16219

Metrics for layer 9:
  pearson_correlation: -0.0131
  kl_divergence: -89.5124
  ssim: 0.0413
  iou: 0.1529
Layer 9 metrics:
  pearson_correlation: -0.0131
  kl_divergence: -89.5124
  ssim: 0.0413
  iou: 0.1529

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.37188, std: 0.18395

Metrics for layer 10:
  pearson_correlation: -0.0059
  kl_divergence: -2.4972
  ssim: 0.0320
  iou: 0.1264
Layer 10 metrics:
  pearson_correlation: -0.0059
  kl_divergence: -2.4972
  ssim: 0.0320
  iou: 0.1264

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.52158, std: 0.17186

Metrics for layer 11:
  pearson_correlation: 0.0483
  kl_divergence: -21.2816
  ssim: 0.0845
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: 0.0483
  kl_divergence: -21.2816
  ssim: 0.0845
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.48550, std: 0.19800

Metrics for layer 12:
  pearson_correlation: 0.0083
  kl_divergence: -17.5790
  ssim: 0.0582
  iou: 0.1136
Layer 12 metrics:
  pearson_correlation: 0.0083
  kl_divergence: -17.5790
  ssim: 0.0582
  iou: 0.1136
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_1_strength_0.8_results.npy

Processing attention strength 1.2
Attention vector: [0.  0.  0.  0.  0.  0.  0.  0.  0.  1.2 0.  0.  0. ]

Processing batch 1/2
Attention strength: 1.2
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.44872, std: 0.12452

Metrics for layer 0:
  pearson_correlation: 0.0025
  kl_divergence: -4741.0269
  ssim: 0.0459
  iou: 0.1446
Layer 0 metrics:
  pearson_correlation: 0.0025
  kl_divergence: -4741.0269
  ssim: 0.0459
  iou: 0.1446

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.06079, std: 0.06400
Attention - min: 0.00000, max: 1.00000, mean: 0.43562, std: 0.12272

Metrics for layer 1:
  pearson_correlation: -0.0077
  kl_divergence: -4636.1538
  ssim: 0.0456
  iou: 0.1415
Layer 1 metrics:
  pearson_correlation: -0.0077
  kl_divergence: -4636.1538
  ssim: 0.0456
  iou: 0.1415

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.45950, std: 0.13206

Metrics for layer 2:
  pearson_correlation: -0.0009
  kl_divergence: -1394.1514
  ssim: 0.0529
  iou: 0.1397
Layer 2 metrics:
  pearson_correlation: -0.0009
  kl_divergence: -1394.1514
  ssim: 0.0529
  iou: 0.1397

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07963, std: 0.07093
Attention - min: 0.00000, max: 1.00000, mean: 0.46009, std: 0.14165

Metrics for layer 3:
  pearson_correlation: 0.0027
  kl_divergence: -1386.6693
  ssim: 0.0492
  iou: 0.1447
Layer 3 metrics:
  pearson_correlation: 0.0027
  kl_divergence: -1386.6693
  ssim: 0.0492
  iou: 0.1447

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.46318, std: 0.13407

Metrics for layer 4:
  pearson_correlation: -0.0077
  kl_divergence: -360.8972
  ssim: 0.0629
  iou: 0.1487
Layer 4 metrics:
  pearson_correlation: -0.0077
  kl_divergence: -360.8972
  ssim: 0.0629
  iou: 0.1487

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.51636, std: 0.14508

Metrics for layer 5:
  pearson_correlation: 0.0148
  kl_divergence: -404.6835
  ssim: 0.0574
  iou: 0.1479
Layer 5 metrics:
  pearson_correlation: 0.0148
  kl_divergence: -404.6835
  ssim: 0.0574
  iou: 0.1479

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11024, std: 0.10024
Attention - min: 0.00000, max: 1.00000, mean: 0.50989, std: 0.15562

Metrics for layer 6:
  pearson_correlation: -0.0164
  kl_divergence: -393.3237
  ssim: 0.0408
  iou: 0.1379
Layer 6 metrics:
  pearson_correlation: -0.0164
  kl_divergence: -393.3237
  ssim: 0.0408
  iou: 0.1379

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.43239, std: 0.16356

Metrics for layer 7:
  pearson_correlation: -0.0072
  kl_divergence: -77.6691
  ssim: 0.0519
  iou: 0.1264
Layer 7 metrics:
  pearson_correlation: -0.0072
  kl_divergence: -77.6691
  ssim: 0.0519
  iou: 0.1264

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.51175, std: 0.13683

Metrics for layer 8:
  pearson_correlation: -0.0355
  kl_divergence: -94.6722
  ssim: 0.0348
  iou: 0.1667
Layer 8 metrics:
  pearson_correlation: -0.0355
  kl_divergence: -94.6722
  ssim: 0.0348
  iou: 0.1667

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.12111, std: 0.11528
Attention - min: 0.00000, max: 1.00000, mean: 0.41679, std: 0.14976

Metrics for layer 9:
  pearson_correlation: 0.0226
  kl_divergence: -70.8542
  ssim: 0.0818
  iou: 0.1529
Layer 9 metrics:
  pearson_correlation: 0.0226
  kl_divergence: -70.8542
  ssim: 0.0818
  iou: 0.1529

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.43538, std: 0.18259

Metrics for layer 10:
  pearson_correlation: -0.0084
  kl_divergence: -11.7246
  ssim: 0.0249
  iou: 0.1264
Layer 10 metrics:
  pearson_correlation: -0.0084
  kl_divergence: -11.7246
  ssim: 0.0249
  iou: 0.1264

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.52580, std: 0.17745

Metrics for layer 11:
  pearson_correlation: 0.0369
  kl_divergence: -23.5803
  ssim: 0.0262
  iou: 0.1395
Layer 11 metrics:
  pearson_correlation: 0.0369
  kl_divergence: -23.5803
  ssim: 0.0262
  iou: 0.1395

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.16107, std: 0.13879
Attention - min: 0.00000, max: 1.00000, mean: 0.44906, std: 0.18802

Metrics for layer 12:
  pearson_correlation: -0.1374
  kl_divergence: -14.0504
  ssim: -0.0797
  iou: 0.0889
Layer 12 metrics:
  pearson_correlation: -0.1374
  kl_divergence: -14.0504
  ssim: -0.0797
  iou: 0.0889
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.06079, std=0.06400
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_0_strength_1.2_results.npy

Processing batch 2/2
Attention strength: 1.2
Generating tuning curve attention maps...
Successfully generated 13 attention maps
Computing saliency maps...
Saliency statistics: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0
Saliency map shape: (1, 224, 224)
Non-zero elements: 50175
Value range: 0.0 - 1.0

=== Debug Shapes After saliency computation ===
Saliency map shape: (1, 224, 224)
Attention map 0 shape: (1, 224, 224, 64)
Attention map 1 shape: (1, 224, 224, 64)
Attention map 2 shape: (1, 112, 112, 128)
Attention map 3 shape: (1, 112, 112, 128)
Attention map 4 shape: (1, 56, 56, 256)
Attention map 5 shape: (1, 56, 56, 256)
Attention map 6 shape: (1, 56, 56, 256)
Attention map 7 shape: (1, 28, 28, 512)
Attention map 8 shape: (1, 28, 28, 512)
Attention map 9 shape: (1, 28, 28, 512)
Attention map 10 shape: (1, 14, 14, 512)
Attention map 11 shape: (1, 14, 14, 512)
Attention map 12 shape: (1, 14, 14, 512)
Successfully generated saliency maps
Comparing saliency and attention maps...

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.44441, std: 0.11370

Metrics for layer 0:
  pearson_correlation: -0.0024
  kl_divergence: -3868.7646
  ssim: 0.0349
  iou: 0.1411
Layer 0 metrics:
  pearson_correlation: -0.0024
  kl_divergence: -3868.7646
  ssim: 0.0349
  iou: 0.1411

After resizing:
Target shape: (224, 224)
Saliency map shape: (224, 224)
Attention map shape: (224, 224)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.03700, std: 0.03738
Attention - min: 0.00000, max: 1.00000, mean: 0.43995, std: 0.12746

Metrics for layer 1:
  pearson_correlation: 0.0038
  kl_divergence: -3829.6509
  ssim: 0.0301
  iou: 0.1471
Layer 1 metrics:
  pearson_correlation: 0.0038
  kl_divergence: -3829.6509
  ssim: 0.0301
  iou: 0.1471

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.44124, std: 0.13203

Metrics for layer 2:
  pearson_correlation: -0.0162
  kl_divergence: -1324.4047
  ssim: 0.0481
  iou: 0.1350
Layer 2 metrics:
  pearson_correlation: -0.0162
  kl_divergence: -1324.4047
  ssim: 0.0481
  iou: 0.1350

After resizing:
Target shape: (112, 112)
Saliency map shape: (112, 112)
Attention map shape: (112, 112)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07117, std: 0.06162
Attention - min: 0.00000, max: 1.00000, mean: 0.43921, std: 0.12690

Metrics for layer 3:
  pearson_correlation: 0.0269
  kl_divergence: -1333.2000
  ssim: 0.0531
  iou: 0.1470
Layer 3 metrics:
  pearson_correlation: 0.0269
  kl_divergence: -1333.2000
  ssim: 0.0531
  iou: 0.1470

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.47622, std: 0.13793

Metrics for layer 4:
  pearson_correlation: 0.0297
  kl_divergence: -389.3981
  ssim: 0.0633
  iou: 0.1693
Layer 4 metrics:
  pearson_correlation: 0.0297
  kl_divergence: -389.3981
  ssim: 0.0633
  iou: 0.1693

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.51409, std: 0.13216

Metrics for layer 5:
  pearson_correlation: -0.0122
  kl_divergence: -416.2969
  ssim: 0.0469
  iou: 0.1289
Layer 5 metrics:
  pearson_correlation: -0.0122
  kl_divergence: -416.2969
  ssim: 0.0469
  iou: 0.1289

After resizing:
Target shape: (56, 56)
Saliency map shape: (56, 56)
Attention map shape: (56, 56)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.11423, std: 0.10009
Attention - min: 0.00000, max: 1.00000, mean: 0.44484, std: 0.12246

Metrics for layer 6:
  pearson_correlation: -0.0005
  kl_divergence: -364.9794
  ssim: 0.0709
  iou: 0.1454
Layer 6 metrics:
  pearson_correlation: -0.0005
  kl_divergence: -364.9794
  ssim: 0.0709
  iou: 0.1454

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.45566, std: 0.15988

Metrics for layer 7:
  pearson_correlation: -0.0264
  kl_divergence: -80.8055
  ssim: 0.0346
  iou: 0.1362
Layer 7 metrics:
  pearson_correlation: -0.0264
  kl_divergence: -80.8055
  ssim: 0.0346
  iou: 0.1362

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.49547, std: 0.16016

Metrics for layer 8:
  pearson_correlation: 0.0228
  kl_divergence: -88.8453
  ssim: 0.0505
  iou: 0.1598
Layer 8 metrics:
  pearson_correlation: 0.0228
  kl_divergence: -88.8453
  ssim: 0.0505
  iou: 0.1598

After resizing:
Target shape: (28, 28)
Saliency map shape: (28, 28)
Attention map shape: (28, 28)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.07594, std: 0.07019
Attention - min: 0.00000, max: 1.00000, mean: 0.44613, std: 0.15434

Metrics for layer 9:
  pearson_correlation: 0.0262
  kl_divergence: -82.4324
  ssim: 0.0412
  iou: 0.1399
Layer 9 metrics:
  pearson_correlation: 0.0262
  kl_divergence: -82.4324
  ssim: 0.0412
  iou: 0.1399

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.52234, std: 0.18203

Metrics for layer 10:
  pearson_correlation: 0.0753
  kl_divergence: -21.2889
  ssim: 0.1418
  iou: 0.1807
Layer 10 metrics:
  pearson_correlation: 0.0753
  kl_divergence: -21.2889
  ssim: 0.1418
  iou: 0.1807

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.43525, std: 0.20368

Metrics for layer 11:
  pearson_correlation: -0.0247
  kl_divergence: -10.3514
  ssim: -0.0783
  iou: 0.1136
Layer 11 metrics:
  pearson_correlation: -0.0247
  kl_divergence: -10.3514
  ssim: -0.0783
  iou: 0.1136

After resizing:
Target shape: (14, 14)
Saliency map shape: (14, 14)
Attention map shape: (14, 14)

Normalized maps statistics:
Saliency - min: 0.00000, max: 1.00000, mean: 0.18658, std: 0.16890
Attention - min: 0.00000, max: 1.00000, mean: 0.39809, std: 0.16676

Metrics for layer 12:
  pearson_correlation: 0.0456
  kl_divergence: -6.1033
  ssim: 0.0789
  iou: 0.1529
Layer 12 metrics:
  pearson_correlation: 0.0456
  kl_divergence: -6.1033
  ssim: 0.0789
  iou: 0.1529
Analyzing attention effects...
Computing baseline responses...
Computing responses with attention...

Creating feed dict:

Creating feed dict:
Successfully analyzed attention effects
Generating visualizations...
Saliency (final) stats at visualization: min=0.00000, max=1.00000, mean=0.03700, std=0.03738
Saved visualization for image 0
Saved batch results to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9/batch_1_strength_1.2_results.npy

Analysis complete! Results saved to /scratch/hy2611/CNN_attention/Data/VGG16/attention_results_cat5_layer9
